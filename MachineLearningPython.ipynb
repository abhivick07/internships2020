{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network from scratch\n",
    "\n",
    "This python notebook contains the assignment questions for candidates who've applied for internships with us. Unlike the Java assignment, the python one requires candidates to share code with us. The assignment revolves around neural networks - the model architecture used for deep learning based tasks. You would be tested on some of the concepts of Neural Networks. In fact, we will be guiding you through the process of coding your own neural network in python from scratch. \n",
    "\n",
    "The places where you need to add code are marked with the word [TASK] In all there are 9 tasks throughout the notebook. Once you've made changes to the code below, you can send us your notebook files to vikash@kiotalabs.com or aditya@kiotalabs.com. Hope you all have fun setting it up.\n",
    "\n",
    "**Please note**: this assignment has borrowed code from similar tutorials that exist elsewhere. We could add references to give credit to the original authors, but that would jeopardize the selection process which relies on the following questions.  \n",
    "\n",
    "Let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots in notebook \n",
    "%matplotlib inline\n",
    "# Define plot's default figure size\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we'll read the input dataset which the Neural Network will use to train a classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the datasets\n",
    "train = pd.read_csv(\"ann_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.array(train.iloc[:,0:2]), np.array(train.iloc[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the dataset and see how it is\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.BuGn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data points have been assigned one of two labels. The task of the classifier is to model the training data, so that it can assign a binary value to samples in the test set. Let's start with the simplest functions.  \n",
    "\n",
    "**Function to generate a random number, given two numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "# calculate a random number where:  a <= rand < b\n",
    "def rand(a, b):\n",
    "    return (b-a)*random.random() + a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to generate a matrix with given dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a matrix \n",
    "def makeMatrix(I, J, fill=0.0):\n",
    "    return np.zeros([I,J])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Implement the sigmoid function. It would be used as the activation function for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the sigmoid function below\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Implement the derivative of the sigmoid function in terms of the output variable. If the output of the sigmoid function in `y` then the function should take `y` as an argument and return the derivative of the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of our sigmoid function, in terms of the output (i.e. y)\n",
    "def dsigmoid(y):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the sigmoid function is used as the activation function for the nodes in the hidden layer of the neural network. A simple neural network that has atleast one hidden layer between the input and the output layer is called a **Multi Layer Perceptron**. We will be creating an MLP from scratch in the following section. \n",
    "\n",
    "To keep things simple, our MLP will have only one hidden layer. We start by defining the variables we need to lay out the architecture - these include \n",
    "1. the number of nodes in the input layer, our single hidden layer and the output layer. \n",
    "2. The initial weights to be used for the different layers. These weights will be randomly initiated.\n",
    "\n",
    "The architecture defined above is captured in the following class which we'll be using for our ANN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, ni, nh, no):\n",
    "        # number of nodes in the input, hidden, and output layer\n",
    "        self.ni = ni + 1 # +1 for bias node\n",
    "        self.nh = nh\n",
    "        self.no = no\n",
    "\n",
    "        # activations for nodes\n",
    "        self.ai = [1.0]*self.ni\n",
    "        self.ah = [1.0]*self.nh\n",
    "        self.ao = [1.0]*self.no\n",
    "\n",
    "        # create weights\n",
    "        self.wi = makeMatrix(self.ni, self.nh)\n",
    "        self.wo = makeMatrix(self.nh, self.no)\n",
    "\n",
    "        # set them to random vaules\n",
    "        self.wi = rand(-0.2, 0.2, size=self.wi.shape)\n",
    "        self.wo = rand(-2.0, 2.0, size=self.wo.shape)\n",
    "\n",
    "        # last change in weights for momentum   \n",
    "        self.ci = makeMatrix(self.ni, self.nh)\n",
    "        self.co = makeMatrix(self.nh, self.no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this class, we need to add the activation component and the backpropogation mechanism. Luckily we already have part of the function defined earlier as the **sigmoid** function. We'll be using it below as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(self, inputs):\n",
    "\n",
    "    if len(inputs) != self.ni-1:\n",
    "        print(inputs)\n",
    "        raise ValueError('wrong number of inputs')\n",
    "\n",
    "    # input activations\n",
    "    for i in range(self.ni-1):\n",
    "        self.ai[i] = inputs[i]\n",
    "\n",
    "    # hidden activations\n",
    "    for j in range(self.nh):\n",
    "        sum_h = 0.0\n",
    "        for i in range(self.ni):\n",
    "            sum_h += self.ai[i] * self.wi[i][j]\n",
    "        self.ah[j] = sigmoid(sum_h)\n",
    "\n",
    "    # output activations\n",
    "    for k in range(self.no):\n",
    "        sum_o = 0.0\n",
    "        for j in range(self.nh):\n",
    "            sum_o += self.ah[j] * self.wo[j][k]\n",
    "        self.ao[k] = sigmoid(sum_o)\n",
    "\n",
    "    return self.ao[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the backpropagation code, we need to calculate the error terms for each node as well as the weight update step. The following code has the implementation for the output layer. You are required to add the implementation for the hidden layer to the same code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backPropagate(self, targets, N, M):\n",
    "\n",
    "    if len(targets) != self.no:\n",
    "        print(targets)\n",
    "        raise ValueError('wrong number of target values')\n",
    "\n",
    "    # calculate error terms for output\n",
    "    output_deltas = np.zeros(self.no)\n",
    "    for k in range(self.no):\n",
    "        error = targets[k]-self.ao[k]\n",
    "        output_deltas[k] = dsigmoid(self.ao[k]) * error\n",
    "\n",
    "    # calculate error terms for hidden\n",
    "    hidden_deltas =np.zeros(self.nh)\n",
		  "    for j in range(self.nh):\n",
    "        error=0.0\n",
			 "        for k in range(self.no):\n",
				"            error+=self.who[j][k]*output_deltas[k]\n",
    "        hidden_deltas[j]= error * dsigmoid(self.ah[j])\n",
    "    \n",
    "    # update output weights\n",
    "    for j in range(self.nh):\n",
    "        for k in range(self.no):\n",
    "            change = output_deltas[k] * self.ah[j]\n",
    "            self.wo[j][k] += N*change + \n",
    "                             M*self.co[j][k]\n",
    "            self.co[j][k] = change\n",
    "\n",
    "    # update input weights\n",
    "    for i in range(self.ni):\n",
    "        for j in range(self.nh):
    "            change = hidden_deltas[j]*self.ai[i]\n",
    "            self.wi[i][j] = self.wi[i][j] + N*change + M*self.ci[i][j]\n",
    "            self.ci[i][j] = change\n",
    "    \n",
    "    # calculate error\n",
    "    error = 0.0\n",
    "    for k in range(len(targets)):\n",
    "        error += 0.5*(targets[k]-self.ao[k])**2\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together \n",
    "In the following code, we need to include the two functions written above - `activate` and `backPropagate` in the `MLP` class defined earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all together\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, ni, nh, no):\n",
    "        # number of input, hidden, and output nodes\n",
    "        self.ni = ni + 1 # +1 for bias node\n",
    "        self.nh = nh\n",
    "        self.no = no\n",
    "\n",
    "        # activations for nodes\n",
    "        self.ai = [1.0]*self.ni\n",
    "        self.ah = [1.0]*self.nh\n",
    "        self.ao = [1.0]*self.no\n",
    "        \n",
    "        # create weights\n",
    "        self.wi = makeMatrix(self.ni, self.nh)\n",
    "        self.wo = makeMatrix(self.nh, self.no)\n",
    "        \n",
    "        # set them to random vaules\n",
    "        for i in range(self.ni):\n",
    "            for j in range(self.nh):\n",
    "                self.wi[i][j] = rand(-0.2, 0.2)\n",
    "        for j in range(self.nh):\n",
    "            for k in range(self.no):\n",
    "                self.wo[j][k] = rand(-2.0, 2.0)\n",
    "\n",
    "        # last change in weights for momentum   \n",
    "        self.ci = makeMatrix(self.ni, self.nh)\n",
    "        self.co = makeMatrix(self.nh, self.no)\n",
    "        \n",
    "\n",
    "    def backPropagate(self, targets, N, M):\n",
    "        if len(targets) != self.no:\n",
    "            print(targets)\n",
    "            raise ValueError('wrong number of target values')\n",
    "\n",
    "        # calculate error terms for output\n",
    "        output_deltas = np.zeros(self.no)\n",
    "        for k in range(self.no):\n",
    "            error = targets[k]-self.ao[k]\n",
    "            output_deltas[k] = dsigmoid(self.ao[k]) * error\n",
    "\n",
    "        # calculate error terms for hidden\n",
    "        hidden_deltas =np.zeros(self.nh)\n",
		  "        for j in range(self.nh):\n",
    "            error=0.0\n",
			 "            for k in range(self.no):\n",
				"                error+=self.who[j][k]*output_deltas[k]\n",
    "            hidden_deltas[j]= error * dsigmoid(self.ah[j])\n",
    "    \n",
    "        # update output weights\n",
    "        for j in range(self.nh):\n",
    "            for k in range(self.no):\n",
    "                change = output_deltas[k] * self.ah[j]\n",
    "                self.wo[j][k] += N*change + \n",
    "                             M*self.co[j][k]\n",
    "            self.co[j][k] = change\n",
    "\n",
    "        # update input weights\n",
    "        for i in range(self.ni):\n",
    "            for j in range(self.nh):
    "                change = hidden_deltas[j]*self.ai[i]\n",
    "                self.wi[i][j] = self.wi[i][j] + N*change + M*self.ci[i][j]\n",
    "                self.ci[i][j] = change\n",
    "    \n",
    "        # calculate error\n",
    "        error = 0.0\n",
    "        for k in range(len(targets)):\n",
    "            error += 0.5*(targets[k]-self.ao[k])**2\n",
    "        return error"

    "\n",
    "\n",
    "    def test(self, patterns):\n",
    "        self.predict = np.empty([len(patterns), self.no])\n",
    "        for i, p in enumerate(patterns):\n",
    "            self.predict[i] = self.activate(p)\n",
    "            #self.predict[i] = self.activate(p[0])\n",
    "            \n",
    "    def activate(self, inputs):\n",
    "        if len(inputs) != self.ni-1:\n",
    "            print(inputs)\n",
    "            raise ValueError('wrong number of inputs')\n",
    "\n",
    "        # input activations\n",
    "        for i in range(self.ni-1):\n",
    "            self.ai[i] = inputs[i]\n",
    "\n",
    "        # hidden activations\n",
    "        for j in range(self.nh):\n",
    "            sum_h = 0.0\n",
    "            for i in range(self.ni):\n",
    "                sum_h += self.ai[i] * self.wi[i][j]\n",
    "            self.ah[j] = sigmoid(sum_h)\n",
    "\n",
    "        # output activations\n",
    "        for k in range(self.no):\n",
    "            sum_o = 0.0\n",
    "            for j in range(self.nh):\n",
    "                sum_o += self.ah[j] * self.wo[j][k]\n",
    "            self.ao[k] = sigmoid(sum_o)\n",
    "\n",
    "        return self.ao[:]"
    "\n",
    "    def train(self, patterns, iterations=1000, N=0.5, M=0.1):\n",
    "        # N: learning rate\n",
    "        # M: momentum factor\n",
    "        patterns = list(patterns)\n",
    "        for i in range(iterations):\n",
    "            error = 0.0\n",
    "            for p in patterns:\n",
    "                inputs = p[0]\n",
    "                targets = p[1]\n",
    "                self.activate(inputs)\n",
    "                error += self.backPropagate([targets], N, M)\n",
    "            if i % 5 == 0:\n",
    "                print('error in interation %d : %-.5f' % (i,error))\n",
    "            print('Final training error: %-.5f' % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a network with two inputs, one hidden, and one output nodes\n",
    "ann = MLP(2, 1, 1)\n",
    "\n",
    "%timeit -n 1 -r 1 ann.train(zip(X,y), iterations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on training dataset and measuring in-sample accuracy¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 1 -r 1 ann.test(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(data=np.array([y, np.ravel(ann.predict)]).T, \n",
    "                          columns=[\"actual\", \"prediction\"])\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(prediction.prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot a decision boundary.\n",
    "# This generates the contour plot to show the decision boundary visually\n",
    "def plot_decision_boundary(nn_model):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), \n",
    "                         np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    nn_model.test(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = nn_model.predict\n",
    "    Z[Z>=0.5] = 1\n",
    "    Z[Z<0.5] = 0\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=40,  c=y, cmap=plt.cm.BuGn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(ann)\n",
    "plt.title(\"Our initial model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not happy with the accuracy achieved by our neural network, you can try changing the architecture of the neural network. Try to train the neural network with 10 units in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TASK 7] Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in practice you would never need to handcraft a neural network. There are amazing frameworks which do most of the heavy lifting for you. It's possible that some of you may already have experience with such framework and are doing highly significant work based on machine learning. It would be unfair to not let you share some of your experience. \n",
    "As you know all these tasks and questions are **optional**, so you may choose to skip all the tasks above, and take up some of the problems below. For this assignment, we'll choose Keras on top of Tensorflow. If you have experience with some other framework, it shouldn't be very hard to use this one. In fact, we're including the steps to get it up and running\n",
    "\n",
    "--- \n",
    "\n",
    "### Install Keras\n",
    "\n",
    "As per [this](https://keras.io/), before installing Keras, you need to install one of the backends. So we'll install Tensorflow then Keras\n",
    "```\n",
    "pip install --upgrade pip\n",
    "pip install tensorflow\n",
    "pip install keras\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_Train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_Test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "## [TASK 8] Add code to set up the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, Y_Train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model \n",
    "evaluation = model.evaluate(X_test, Y_Test, verbose=1)\n",
    "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron model\n",
    "\n",
    "The same task can be carried out using a MLP like the one developed earlier. However, with Keras the model can be set up in much fewer lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# [TASK 9]\n",
    "# Add two hidden layers with 625 nodes each, with normal weights initialization, and sigmoid function for activation\n",
    "# Add the output layer with 10 nodes and softmax function for activation\n",
    "# Use stochastic gradient descent for optimization with a learning rate of 0.05 and loss being calculated using `categorical_crossentropy`\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "history = model.fit(X_train, Y_Train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluation = model.evaluate(X_test, Y_Test, verbose=1)\n",
    "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks. You've come to the end of the Python assignment. Looking forward to your submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tuto",
   "language": "python",
   "name": "tuto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
